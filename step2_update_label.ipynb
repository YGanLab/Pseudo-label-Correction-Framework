{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import cv2\n",
    "from scipy.misc import imsave\n",
    "import scipy.ndimage as ndimage\n",
    "from unet import UNet\n",
    "from loss import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import heapq\n",
    "from torch.autograd import Variable\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 1\n",
    "def get_info(filenames, ext, root):\n",
    "    images = []\n",
    "    for filename in filenames :\n",
    "        filepath = os.path.join(root,filename)\n",
    "        if ext == '.npy':\n",
    "            image = np.load(filepath)\n",
    "        elif ext == '.JPG' or ext == '.tif' or ext =='.png' or ext =='.tiff':\n",
    "            image = ndimage.imread(filepath)\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "def get_data(directory,ext):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    \n",
    "    root_path = \"\"\n",
    "    filenames = [f for f in listdir(directory) if isfile(join(directory, f)) and f != '.DS_Store']\n",
    "    filenames = sorted(filenames)\n",
    "    return filenames, get_info(filenames, ext, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "filenames_im, images = get_data(cwd+'\\medical_images\\\\oct_images\\\\Train','.png')\n",
    "filenames_lb, lbs = get_data(cwd+'\\medical_images\\\\training_set','.png')\n",
    "print(len(filenames_im))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(n_channels=1, n_classes=2).cuda()\n",
    "#model = nn.DataParallel(model)\n",
    "PATH = os.getcwd() + '\\\\models\\\\model_noise_75_1_8\\\\1it\\\\seg_module.model'\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from relaynet_pytorch.net_api.losses import CombinedLoss\n",
    "loss_func=CombinedLoss()\n",
    "total_loss = []\n",
    "\n",
    "for j in range(len(filenames_im)):\n",
    "    im = images[j].copy()\n",
    "    w = np.ones([1,1,im.shape[0], im.shape[1]])\n",
    "    label = lbs[j].reshape((1,im.shape[0], -1))\n",
    "    im = im.reshape((1,1,im.shape[0], -1))    \n",
    "    im = torch.from_numpy(im) \n",
    "    w = torch.from_numpy(w) \n",
    "    label = torch.from_numpy(label)\n",
    "\n",
    "    y = Variable(label).cuda().float()\n",
    "    X = Variable(im).cuda().float()\n",
    "    w = Variable(w).cuda().float()\n",
    "    output = model(X)\n",
    "    output2 = output.data.cpu().numpy()\n",
    "    output2 = np.squeeze(output2[0])\n",
    "    output2 = np.argmax(output2,0)\n",
    "    output2 = np.squeeze(output2)\n",
    "\n",
    "    loss = loss_func(output.float(), y.float(), w.float(), num_classes = 2)\n",
    "    total_loss.append(loss[2].item())\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the loss values\n",
    "nlevel = 0.75\n",
    "max_indexes = map(total_loss.index, heapq.nlargest(int(124*nlevel), total_loss))\n",
    "max_index_k = list(max_indexes)\n",
    "#print(len(max_index_k), max_index_k)\n",
    "#for index_k in max_index_k:\n",
    "    #print(total_loss[index_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "check_directory = cwd\n",
    "filenames_lb, lbs = get_data(cwd+'\\medical_images\\\\\\\\training_set','.png')\n",
    "filenames_pred, pred = get_data(cwd+'\\\\medical_images\\\\prediction','.png')\n",
    "filenames_gt, gts = get_data(cwd+'\\medical_images\\\\odd_gt','.png')\n",
    "gts = np.array(gts)\n",
    "gts[gts >1] =1\n",
    "print(len(filenames_lb))\n",
    "print(len(filenames_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(filenames_im)):\n",
    "    print(filenames_im[j])\n",
    "    if j in max_index_k:\n",
    "        im = images[j].copy()\n",
    "        im = im*1.0/np.max(im)\n",
    "        im = im.reshape((1,1,im.shape[0], -1))\n",
    "        im = torch.from_numpy(im)\n",
    "        X = Variable(im).cuda().float()\n",
    "        out_map = []\n",
    "        number_dropout = 5\n",
    "        for k in range(number_dropout):\n",
    "            output = model(X)\n",
    "            output2 = output.data.cpu().numpy()\n",
    "            output2 = np.squeeze(output2[0])\n",
    "\n",
    "            output2 = np.argmax(output2,0)\n",
    "            output2 = np.squeeze(output2)\n",
    "            pred = cv2.imread(cwd + '\\\\medical_images\\\\prediction\\\\' + filenames_im[j], 0)\n",
    "            out_result = np.zeros(pred.shape)\n",
    "            out_result[pred != output2] = 1.0\n",
    "            out_map.append(out_result)\n",
    "\n",
    "        out_map = np.array(out_map)\n",
    "        out_map = np.sum(out_map, 0)/number_dropout\n",
    "        uncertainty_map = np.zeros(out_map.shape)\n",
    "        uncertainty_map[out_map > 0.2] = 1\n",
    "\n",
    "        noisy_lb = lbs[j].copy()\n",
    "        noisy_lb[noisy_lb > 1] = 1\n",
    "        gt = gts[j].copy()\n",
    "        gt[gt > 1] =1\n",
    "        re = np.concatenate((uncertainty_map, out_map), 1) \n",
    "        plt.imshow(re)\n",
    "        plt.show()\n",
    "        \n",
    "        if len(np.unique(gt-noisy_lb)) == 1:\n",
    "            print('clean')\n",
    "        else:\n",
    "            print('noisy')\n",
    "        pred[uncertainty_map == 1] = noisy_lb[uncertainty_map == 1] \n",
    "        re = np.concatenate((noisy_lb, pred, gt), 1)\n",
    "        plt.imshow(re)\n",
    "        plt.show()\n",
    "    else:\n",
    "        pred = lbs[j].copy()\n",
    "    save_path = cwd + '\\\\medical_images\\\\updated_label\\\\' + filenames_im[j]\n",
    "    cv2.imwrite(save_path, pred*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
